{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/03/17 21:40:05 WARN Utils: Your hostname, DESKTOP-Q5PPUKU resolves to a loopback address: 127.0.1.1; using 172.30.227.126 instead (on interface eth0)\n",
      "25/03/17 21:40:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/17 21:40:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/17 21:40:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.5.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.4MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import requests\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "init_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "filename = \"yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "with requests.get(init_url) as response:\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        df = spark.read.parquet(filename)\n",
    "        df.repartition(4).write.parquet(f'./parquet')\n",
    "files = [f for f in os.listdir(\"./parquet/\") if f.endswith(\".parquet\")]\n",
    "size = []\n",
    "for file in files:\n",
    "    size.append(os.path.getsize(f'./parquet/{file}'))\n",
    "size = np.array(size)\n",
    "print(f'{round(size.mean()/(1024 * 1024), 2)}MB')\n",
    "### remove file parquet from url\n",
    "os.remove(filename)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|day|total_trips|\n",
      "+---+-----------+\n",
      "| 15|     128893|\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import requests\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "init_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "filename = \"yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "with requests.get(init_url) as response:\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        df = spark.read.parquet(filename)\n",
    "        df.createOrReplaceTempView('yellow_trip')\n",
    "        spark.sql(\"\"\"\n",
    "                  SELECT \n",
    "                    extract(day from tpep_pickup_datetime) as day,\n",
    "                    count(*) AS total_trips\n",
    "                  FROM yellow_trip WHERE tpep_pickup_datetime >= '2024-10-15 00:00:00' AND tpep_pickup_datetime < '2024-10-16 00:00:00' GROUP BY 1 ORDER BY day;\n",
    "                  \"\"\").show()\n",
    "\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:=============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|hour_difference|\n",
      "+---------------+\n",
      "|            162|\n",
      "|            143|\n",
      "|            137|\n",
      "|            114|\n",
      "|             89|\n",
      "|             89|\n",
      "|             70|\n",
      "|             67|\n",
      "|             66|\n",
      "|             46|\n",
      "|             42|\n",
      "|             38|\n",
      "|             33|\n",
      "|             26|\n",
      "|             25|\n",
      "|             25|\n",
      "|             24|\n",
      "|             23|\n",
      "|             23|\n",
      "|             23|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import requests\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "init_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "filename = \"yellow_tripdata_2024-10.parquet\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "with requests.get(init_url) as response:\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        df = spark.read.parquet(filename)\n",
    "        df.createOrReplaceTempView('yellow_trip')\n",
    "        spark.sql(\"\"\"\n",
    "                  SELECT \n",
    "                    TIMESTAMPDIFF(HOUR, tpep_pickup_datetime, tpep_dropoff_datetime) AS hour_difference\n",
    "                  FROM yellow_trip ORDER BY hour_difference DESC;\n",
    "                  \"\"\").show()\n",
    "\n",
    "os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 122:=====================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|         pickup_zone|total_pickup|\n",
      "+--------------------+------------+\n",
      "|Governor's Island...|           1|\n",
      "|       Rikers Island|           2|\n",
      "|       Arden Heights|           2|\n",
      "|         Jamaica Bay|           3|\n",
      "| Green-Wood Cemetery|           3|\n",
      "|Charleston/Totten...|           4|\n",
      "|   Rossville/Woodrow|           4|\n",
      "|       West Brighton|           4|\n",
      "|Eltingville/Annad...|           4|\n",
      "|       Port Richmond|           4|\n",
      "|         Great Kills|           6|\n",
      "|        Crotona Park|           6|\n",
      "|Heartland Village...|           7|\n",
      "|     Mariners Harbor|           7|\n",
      "|Saint George/New ...|           9|\n",
      "|             Oakwood|           9|\n",
      "|       Broad Channel|          10|\n",
      "|New Dorp/Midland ...|          10|\n",
      "|         Westerleigh|          12|\n",
      "|     Pelham Bay Park|          12|\n",
      "+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import requests\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "yellow_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\"\n",
    "taxi_zone_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\"\n",
    "\n",
    "yellow_file = \"yellow_tripdata_2024-10.parquet\"\n",
    "taxi_zone_file = \"taxi_zone_lookup.csv\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('homework') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "with requests.get(yellow_url) as response:\n",
    "    if response.status_code == 200:\n",
    "        with open(yellow_file, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "with requests.get(taxi_zone_url) as response:\n",
    "    if response.status_code == 200:\n",
    "        with open(taxi_zone_file, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            \n",
    "yellow_df = spark.read.parquet(yellow_file)\n",
    "taxi_zone = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .csv(taxi_zone_file)\n",
    "yellow_df.createOrReplaceTempView('yellow_trip')\n",
    "taxi_zone.createOrReplaceTempView('taxi_zone')\n",
    "spark.sql(\"\"\"\n",
    "            WITH fct_yellow_trip AS (SELECT VendorID,\n",
    "                                            tpep_pickup_datetime as pickup_datetime,\n",
    "                                            tpep_dropoff_datetime as dropoff_datetime,\n",
    "                                            passenger_count,\n",
    "                                            trip_distance,\n",
    "                                            RatecodeID,\n",
    "                                            store_and_fwd_flag,\n",
    "                                            PULocationID,\n",
    "                                            DOLocationID,\n",
    "                                            payment_type,\n",
    "                                            fare_amount,\n",
    "                                            extra,\n",
    "                                            mta_tax,\n",
    "                                            tip_amount,\n",
    "                                            tolls_amount,\n",
    "                                            improvement_surcharge,\n",
    "                                            total_amount,\n",
    "                                            congestion_surcharge,\n",
    "                                            Airport_fee\n",
    "                                    FROM yellow_trip),\n",
    "            dim_taxi_zone AS (SELECT locationid, borough, zone, replace(service_zone, 'Boro', 'Green') as service_zone FROM taxi_zone),\n",
    "            final_table AS (\n",
    "            SELECT \n",
    "                y.VendorID,\n",
    "                y.pickup_datetime,\n",
    "                y.dropoff_datetime,\n",
    "                y.passenger_count,\n",
    "                y.trip_distance,\n",
    "                y.RatecodeID,\n",
    "                y.store_and_fwd_flag,\n",
    "                y.PULocationID,\n",
    "                pz.borough as pickup_borough,\n",
    "                pz.zone as pickup_zone,\n",
    "                y.DOLocationID,\n",
    "                dz.borough as dropoff_borough,\n",
    "                dz.zone as dropoff_zone,\n",
    "                y.payment_type,\n",
    "                y.fare_amount,\n",
    "                y.extra,\n",
    "                y.mta_tax,\n",
    "                y.tip_amount,\n",
    "                y.tolls_amount,\n",
    "                y.improvement_surcharge,\n",
    "                y.total_amount,\n",
    "                y.congestion_surcharge,\n",
    "                y.Airport_fee\n",
    "            FROM fct_yellow_trip AS y\n",
    "            INNER JOIN dim_taxi_zone AS pz\n",
    "            ON (y.PULocationID = pz.locationid)\n",
    "            INNER JOIN dim_taxi_zone AS dz\n",
    "            ON (y.DOLocationID = dz.locationid)\n",
    "            )\n",
    "            SELECT pickup_zone,\n",
    "            count(*) AS total_pickup\n",
    "            FROM final_table GROUP BY 1 ORDER BY 2\n",
    "            \"\"\").show()\n",
    "\n",
    "os.remove(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modif_py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
